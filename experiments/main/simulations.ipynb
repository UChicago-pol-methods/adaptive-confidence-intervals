{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script runs simulations reported in our paper Confidence Intervals for Policy Evaluation in Adaptive Experiments (https://arxiv.org/abs/1911.02768)\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "from sys import argv\n",
    "from random import choice\n",
    "from time import time\n",
    "from os.path import dirname, realpath, join, exists\n",
    "from os import makedirs, chmod\n",
    "from getpass import getuser\n",
    "\n",
    "from adaptive_CI.experiments import run_mab_experiment\n",
    "from adaptive_CI.compute import stick_breaking\n",
    "from adaptive_CI.inference import *\n",
    "from adaptive_CI.weights import *\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Figures\n",
    "\n",
    "This simulation script produces most figures in the paper, with the _exception_ of Figure 1 in the introduction, and its counterpart Figure 13 in Appendix A5.\n",
    "\n",
    "\n",
    "**To non-Stanford members**\n",
    "\n",
    "Each time this script is called, it selects a random configuration (e.g., a signal strength, an experiment horizon, etc) and completes a single simulation using that configuration.\n",
    "\n",
    "In order to produce the figures in our paper, we recommend that this script be run at least $10Ë†5$ times.\n",
    "\n",
    "\n",
    "\n",
    "**To Stanford members with access to the Sherlock cluster**\n",
    "\n",
    "Each time this script is called on sherlock, it selects a random configuration (e.g., a signal strength, an experiment horizon, etc) and completes 200 simulations using that configuration.\n",
    "\n",
    "In order to produce the figures in our paper, we recommend that this script be run a few thousand times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_sherlock():\n",
    "    \"\"\" \n",
    "    Note: This can be ignored by non-Stanford members.\n",
    "\n",
    "    Checks if running on Stanford's Sherlock cluster\n",
    "    \"\"\"\n",
    "    return 'GROUP_SCRATCH' in os.environ\n",
    "\n",
    "\n",
    "def get_sherlock_dir(project, *tail, create=True):\n",
    "    \"\"\"\n",
    "    Note: This can be ignored by non-Stanford members.\n",
    "    \n",
    "    Output consistent folder name in Sherlock.\n",
    "    If create=True and on Sherlock, also makes folder with group permissions.\n",
    "    If create=True and not on Sherlock, does not create anything.\n",
    "\n",
    "    '/scratch/groups/athey/username/project/tail1/tail2/.../tailn'.\n",
    "\n",
    "    >>> get_sherlock_dir('adaptive-inference')\n",
    "    '/scratch/groups/athey/adaptive-inference/vitorh'\n",
    "\n",
    "    >>> get_sherlock_dir('toronto')\n",
    "    '/scratch/groups/athey/toronto/vitorh/'\n",
    "\n",
    "    >>> get_sherlock_dir('adaptive-inference', 'experiments', 'exp_out')\n",
    "    '/scratch/groups/athey/adaptive-inference/vitorh/experiments/exp_out'\n",
    "    \"\"\"\n",
    "    base = join(\"/\", \"scratch\", \"groups\", \"athey\", project, getuser())\n",
    "    path = join(base, *tail)\n",
    "    if not exists(path) and create and on_sherlock():\n",
    "        makedirs(path, exist_ok=True)\n",
    "        # Correct permissions for the whole directory branch\n",
    "        chmod_path = base\n",
    "        chmod(base, 0o775)\n",
    "        for child in tail:\n",
    "            chmod_path = join(chmod_path, child)\n",
    "            chmod(chmod_path, 0o775)\n",
    "    return path\n",
    "\n",
    "\n",
    "def compose_filename(prefix, extension):\n",
    "    \"\"\"\n",
    "    Creates a unique filename based on Github commit id and time.\n",
    "    Useful when running in parallel on server.\n",
    "\n",
    "    INPUT:\n",
    "        - prefix: file name prefix\n",
    "        - extension: file extension\n",
    "\n",
    "    OUTPUT:\n",
    "        - fname: unique filename\n",
    "    \"\"\"\n",
    "    # Tries to find a commit hash\n",
    "    try:\n",
    "        commit = subprocess\\\n",
    "            .check_output(['git', 'rev-parse', '--short', 'HEAD'],\n",
    "                          stderr=subprocess.DEVNULL)\\\n",
    "            .strip()\\\n",
    "            .decode('ascii')\n",
    "    except subprocess.CalledProcessError:\n",
    "        commit = ''\n",
    "\n",
    "    # Other unique identifiers\n",
    "    rnd = str(int(time() * 1e8 % 1e8))\n",
    "    sid = tid = jid = ''\n",
    "    ident = filter(None, [prefix, commit, jid, sid, tid, rnd])\n",
    "    basename = \"_\".join(ident)\n",
    "    fname = f\"{basename}.{extension}\"\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims = 200 if on_sherlock() else 1\n",
    "\n",
    "# DGP specification\n",
    "# ----------------------------------------------------\n",
    "noise_func = 'uniform'\n",
    "truths = {\n",
    "    'nosignal': np.array([1., 1., 1.]),\n",
    "    'lowSNR': np.array([.9, 1., 1.1]),\n",
    "    'highSNR': np.array([.5, 1., 1.5])\n",
    "}\n",
    "if on_sherlock():\n",
    "    Ts = [1_000, 5_000, 10_000, 50_000, 100_000]\n",
    "else:\n",
    "    Ts = [100_000]\n",
    "floor_decays = [.7]\n",
    "initial = 5  # initial number of samples of each arm to do pure exploration\n",
    "exploration = 'TS'\n",
    "noise_scale = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = []\n",
    "df_lambdas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed 18.6770601272583s\n"
     ]
    }
   ],
   "source": [
    "# Run simulations\n",
    "for s in range(num_sims):\n",
    "    if (s+1) % 10 == 0:\n",
    "        print(f'Running simulation {s+1}/{num_sims}')\n",
    "\n",
    "    \"\"\" Experiment configuration \"\"\"\n",
    "    T = choice(Ts)  # number of samples\n",
    "    experiment = choice(list(truths.keys()))\n",
    "    truth = truths[experiment]\n",
    "    K = len(truth)  # number of arms\n",
    "    floor_start = 1/K\n",
    "    floor_decay = choice(floor_decays)\n",
    "\n",
    "    \"\"\" Generate data \"\"\"\n",
    "    noise = np.random.uniform(-noise_scale, noise_scale, size=(T, K))\n",
    "    ys = truth + noise\n",
    "\n",
    "    \"\"\" Run experiment \"\"\"\n",
    "    data = run_mab_experiment(\n",
    "        ys,\n",
    "        initial=initial,\n",
    "        floor_start=floor_start,\n",
    "        floor_decay=floor_decay,\n",
    "        exploration=exploration)\n",
    "\n",
    "    probs = data['probs']\n",
    "    rewards = data['rewards']\n",
    "    arms = data['arms']\n",
    "\n",
    "    \"\"\" Compute AIPW scores \"\"\"\n",
    "    muhat = np.row_stack([np.zeros(K), sample_mean(rewards, arms, K)[:-1]])\n",
    "    scores = aw_scores(rewards, arms, probs, muhat)\n",
    "\n",
    "    \"\"\" Compute weights \"\"\"\n",
    "    # Two-point allocation rate\n",
    "    twopoint_ratio = twopoint_stable_var_ratio(e=probs, alpha=floor_decay)\n",
    "    twopoint_ratio_old = twopoint_stable_var_ratio_old(probs, floor_start, floor_decay)\n",
    "    twopoint_h2es = stick_breaking(twopoint_ratio)\n",
    "    twopoint_h2es_old = stick_breaking(twopoint_ratio_old)\n",
    "    wts_twopoint = np.sqrt(np.maximum(0., twopoint_h2es * probs))\n",
    "\n",
    "    # Other weights: lvdl(constant allocation rate), propscore and uniform\n",
    "    wts_lvdl = np.sqrt(probs)\n",
    "    wts_propscore = probs\n",
    "    wts_uniform = np.ones_like(probs)\n",
    "\n",
    "    \"\"\" Estimate arm values \"\"\"\n",
    "    # for each weighting scheme, return [estimate, S.E, bias, 90%-coverage, t-stat, mse, truth]\n",
    "    stats = dict(\n",
    "        uniform=evaluate_aipw_stats(scores, wts_uniform, truth),\n",
    "        propscore=evaluate_aipw_stats(scores, wts_propscore, truth),\n",
    "        lvdl=evaluate_aipw_stats(scores, wts_lvdl, truth),\n",
    "        two_point=evaluate_aipw_stats(scores, wts_twopoint, truth),\n",
    "        beta_bernoulli=evaluate_beta_bernoulli_stats(rewards, arms, truth, K, floor_decay, alpha=.1),\n",
    "        gamma_exponential=evaluate_gamma_exponential_stats(rewards, arms, truth, K, floor_decay, c=2, expected_noise_variance=1/3, alpha=.1),\n",
    "        sample_mean_naive=evaluate_sample_mean_naive_stats(rewards, arms, truth, K, alpha=.1)\n",
    "    )\n",
    "    \n",
    "    # # add estimates of W_decorrelation\n",
    "    W_name = f'wdecorr_results/W_lambdas_{experiment}-{noise_func}-{T}-{floor_decay}.npz'\n",
    "    try:\n",
    "        W_save = np.load(W_name)  # load presaved W-lambdas\n",
    "        for percentile, W_lambda in zip(W_save['percentiles'], W_save['W_lambdas']):\n",
    "            stats[f'W-decorrelation_{percentile}'] = wdecorr_stats(arms, rewards, K, W_lambda, truth)\n",
    "    except FileNotFoundError:\n",
    "        print(f'Could not find relevant w-decorrelation file {W_name}. Ignoring.')\n",
    "        \n",
    "    \n",
    "    \"\"\" Estimate contrasts \"\"\"\n",
    "    contrasts = dict(\n",
    "        uniform=evaluate_aipw_contrasts(scores, wts_uniform, truth),\n",
    "        propscore=evaluate_aipw_contrasts(scores, wts_propscore, truth),\n",
    "        lvdl=evaluate_aipw_contrasts(scores, wts_lvdl, truth),\n",
    "        two_point=evaluate_aipw_contrasts(scores, wts_twopoint, truth),\n",
    "        beta_bernoulli=evaluate_beta_bernoulli_contrasts(rewards, arms, truth, K, floor_decay, alpha=.1),\n",
    "        gamma_exponential=evaluate_gamma_exponential_contrasts(rewards, arms, truth, K, floor_decay, c=2, expected_noise_variance=1/3, alpha=.1),\n",
    "        sample_mean_naive=evaluate_sample_mean_naive_contrasts(rewards, arms, truth, K, alpha=.1)\n",
    "    )\n",
    "\n",
    "    \n",
    "    \"\"\" Save results \"\"\"\n",
    "    config = dict(\n",
    "        T=T,\n",
    "        K=K,\n",
    "        noise_func=noise_func,\n",
    "        noise_scale=noise_scale,\n",
    "        floor_start=floor_start,\n",
    "        floor_decay=floor_decay,\n",
    "        initial=initial,\n",
    "        dgp=experiment,\n",
    "    )\n",
    "\n",
    "    ratios = dict(\n",
    "        lvdl=np.ones((T, K)) / np.arange(T, 0, -1)[:, np.newaxis],\n",
    "        two_point=twopoint_ratio,\n",
    "    )\n",
    "    \n",
    "    # save lambda values at selected timepoints\n",
    "    saved_timepoints = list(range(0, T, T // 250)) + [T-1]\n",
    "    for ratio in ratios:\n",
    "        ratios[ratio] = ratios[ratio][saved_timepoints, :]\n",
    "    \n",
    "    # tabulate arm values\n",
    "    tabs_stats = []\n",
    "    for method, stat in stats.items():\n",
    "        tab_stats = pd.DataFrame({\"statistic\": [\"estimate\", \"stderr\", \"bias\", \"90% coverage of t-stat\", \"t-stat\", \"mse\", \"CI_width\", \"truth\"] * stat.shape[1],\n",
    "                                  \"policy\": np.repeat(np.arange(K), stat.shape[0]),\n",
    "                                  \"value\":  stat.flatten(order='F'),\n",
    "                                  \"method\": method,\n",
    "                                 **config})\n",
    "        tabs_stats.append(tab_stats)\n",
    "\n",
    "\n",
    "    # tabulate arm contrasts\n",
    "    tabs_contrasts = []\n",
    "    for method, contrast in contrasts.items():\n",
    "        tabs_contrast = pd.DataFrame({\"statistic\": [\"estimate\", \"stderr\", \"bias\", \"90% coverage of t-stat\", \"t-stat\", \"mse\", \"CI_width\", \"truth\"] * contrast.shape[1],\n",
    "                                      \"policy\": np.repeat([f\"(0,{k})\" for k in np.arange(1, K)], contrast.shape[0]),\n",
    "                                      \"value\": contrast.flatten(order='F'),\n",
    "                                      \"method\": method,\n",
    "                                     **config})\n",
    "        tabs_contrasts.append(tabs_contrast)\n",
    "\n",
    "    \n",
    "    df_stats.extend(tabs_stats)\n",
    "    df_stats.extend(tabs_contrasts)\n",
    "    \n",
    "    \n",
    "    \"\"\" Save relevant lambda weights, if applicable \"\"\"\n",
    "    if T == max(Ts):\n",
    "        saved_timepoints = list(range(0, T, T // 500))\n",
    "        lambdas = twopoint_ratio[saved_timepoints] * (T - np.array(saved_timepoints)[:,np.newaxis])\n",
    "        lambdas = {key: value for key, value in enumerate(lambdas.T)}\n",
    "        dfl = pd.DataFrame({**lambdas, **config, 'time': saved_timepoints})\n",
    "        dfl = pd.melt(dfl, id_vars=list(config.keys()) + ['time'], var_name='policy', value_vars=list(range(K)))\n",
    "        df_lambdas.append(dfl)\n",
    "        \n",
    "    print(f\"Time passed {time()-start_time}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break down the output into different chunks, so it will be easier to pick what to load and plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the directory (the first statement here is specific to the Stanford cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if on_sherlock():\n",
    "    write_dir = get_sherlock_dir('adaptive-confidence-intervals', 'simulations', create=True)\n",
    "    print(f\"saving at {write_dir}\")\n",
    "else:\n",
    "     write_dir = join(os.getcwd(), 'results')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.concat(df_stats, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving information about contrasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_contrast = compose_filename(f'contrast', 'pkl')\n",
    "write_path_contrast = os.path.join(write_dir, filename_contrast)\n",
    "\n",
    "df_contrast = df_stats.query(\n",
    "            'policy == \"(0,2)\" and '\n",
    "            'statistic == [\"mse\", \"bias\", \"90% coverage of t-stat\", \"CI_width\"] and '\n",
    "            \"method == ['uniform', 'lvdl', 'two_point',  'sample_mean_naive', 'gamma_exponential', 'W-decorrelation_15']\")\n",
    "df_contrast.to_pickle(write_path_contrast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save information about arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_arms = compose_filename(f'arm', 'pkl')\n",
    "write_path_arms = os.path.join(write_dir, filename_arms)\n",
    "\n",
    "df_arms = df_stats.query(\n",
    "            'policy == [0, 1, 2] and '\n",
    "            'statistic == [\"mse\", \"bias\", \"90% coverage of t-stat\", \"CI_width\"] and '\n",
    "            \"method == ['uniform', 'lvdl', 'two_point',  'sample_mean_naive', 'gamma_exponential', 'W-decorrelation_15']\")\n",
    "df_arms.to_pickle(write_path_arms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save information about \"t-stats\" (i.e., our studentized 'statistics')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_tstats = compose_filename(f'tstat', 'pkl')\n",
    "write_path_tstats = os.path.join(write_dir, filename_tstats)\n",
    "\n",
    "T_max = max(Ts)\n",
    "df_tstats = df_stats.query(\n",
    "    \"method == ['uniform', 'lvdl', 'two_point',  'sample_mean_naive', 'gamma_exponential', 'W-decorrelation_15'] and \"\n",
    "    \"T == @T_max and \"\n",
    "    \"statistic == 't-stat'\"\n",
    ")\n",
    "df_tstats.to_pickle(write_path_tstats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save information about $\\lambda$ behavior, if appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_lambdas = compose_filename(f'lambdas', 'pkl')\n",
    "write_path_lambdas = os.path.join(write_dir, filename_lambdas)\n",
    "if len(df_lambdas) > 0:\n",
    "    df_lambdas = pd.concat(df_lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done.\n"
     ]
    }
   ],
   "source": [
    "print(\"All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  statistic policy     value              method       T  K  \\\n",
      "2                      bias      0 -0.001299             uniform  100000  3   \n",
      "3    90% coverage of t-stat      0  1.000000             uniform  100000  3   \n",
      "5                       mse      0  0.000002             uniform  100000  3   \n",
      "6                  CI_width      0  0.006170             uniform  100000  3   \n",
      "10                     bias      1 -0.078744             uniform  100000  3   \n",
      "..                      ...    ...       ...                 ...     ... ..   \n",
      "206                CI_width      1  0.024587  W-decorrelation_15  100000  3   \n",
      "210                    bias      2  0.016012  W-decorrelation_15  100000  3   \n",
      "211  90% coverage of t-stat      2  1.000000  W-decorrelation_15  100000  3   \n",
      "213                     mse      2  0.000256  W-decorrelation_15  100000  3   \n",
      "214                CI_width      2  0.023323  W-decorrelation_15  100000  3   \n",
      "\n",
      "    noise_func  noise_scale  floor_start  floor_decay  initial       dgp  \n",
      "2      uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "3      uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "5      uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "6      uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "10     uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "..         ...          ...          ...          ...      ...       ...  \n",
      "206    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "210    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "211    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "213    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "214    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "\n",
      "[72 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./results/arm_5c2057b_50786912.pkl', 'rb') as file:\n",
    "    arm = pickle.load(file)\n",
    "\n",
    "print(arm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  statistic policy     value             method       T  K  \\\n",
      "274                    bias  (0,2)  0.145616            uniform  100000  3   \n",
      "275  90% coverage of t-stat  (0,2)  1.000000            uniform  100000  3   \n",
      "277                     mse  (0,2)  0.021204            uniform  100000  3   \n",
      "278                CI_width  (0,2)  0.196190            uniform  100000  3   \n",
      "306                    bias  (0,2)  0.017424               lvdl  100000  3   \n",
      "307  90% coverage of t-stat  (0,2)  1.000000               lvdl  100000  3   \n",
      "309                     mse  (0,2)  0.000304               lvdl  100000  3   \n",
      "310                CI_width  (0,2)  0.019774               lvdl  100000  3   \n",
      "322                    bias  (0,2)  0.015118          two_point  100000  3   \n",
      "323  90% coverage of t-stat  (0,2)  1.000000          two_point  100000  3   \n",
      "325                     mse  (0,2)  0.000229          two_point  100000  3   \n",
      "326                CI_width  (0,2)  0.017991          two_point  100000  3   \n",
      "354                    bias  (0,2)  0.014754  gamma_exponential  100000  3   \n",
      "355  90% coverage of t-stat  (0,2)  1.000000  gamma_exponential  100000  3   \n",
      "357                     mse  (0,2)  0.000218  gamma_exponential  100000  3   \n",
      "358                CI_width  (0,2)  0.046360  gamma_exponential  100000  3   \n",
      "370                    bias  (0,2)  0.014754  sample_mean_naive  100000  3   \n",
      "371  90% coverage of t-stat  (0,2)  0.000000  sample_mean_naive  100000  3   \n",
      "373                     mse  (0,2)  0.000218  sample_mean_naive  100000  3   \n",
      "374                CI_width  (0,2)  0.014475  sample_mean_naive  100000  3   \n",
      "\n",
      "    noise_func  noise_scale  floor_start  floor_decay  initial       dgp  \n",
      "274    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "275    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "277    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "278    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "306    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "307    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "309    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "310    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "322    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "323    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "325    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "326    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "354    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "355    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "357    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "358    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "370    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "371    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "373    uniform          1.0     0.333333          0.7        5  nosignal  \n",
      "374    uniform          1.0     0.333333          0.7        5  nosignal  \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./results/contrast_5c2057b_91845088.pkl', 'rb') as file:\n",
    "    contrast = pickle.load(file)\n",
    "\n",
    "print(contrast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    statistic policy     value              method       T  K noise_func  \\\n",
      "4      t-stat      0 -0.346262             uniform  100000  3    uniform   \n",
      "12     t-stat      1 -0.797223             uniform  100000  3    uniform   \n",
      "20     t-stat      2  1.000150             uniform  100000  3    uniform   \n",
      "52     t-stat      0  0.701460                lvdl  100000  3    uniform   \n",
      "60     t-stat      1 -1.374332                lvdl  100000  3    uniform   \n",
      "68     t-stat      2  0.460526                lvdl  100000  3    uniform   \n",
      "76     t-stat      0  0.170469           two_point  100000  3    uniform   \n",
      "84     t-stat      1 -1.210176           two_point  100000  3    uniform   \n",
      "92     t-stat      2  0.703957           two_point  100000  3    uniform   \n",
      "124    t-stat      0  0.838695   gamma_exponential  100000  3    uniform   \n",
      "132    t-stat      1 -1.875722   gamma_exponential  100000  3    uniform   \n",
      "140    t-stat      2 -0.149979   gamma_exponential  100000  3    uniform   \n",
      "148    t-stat      0  0.838695   sample_mean_naive  100000  3    uniform   \n",
      "156    t-stat      1 -1.875722   sample_mean_naive  100000  3    uniform   \n",
      "164    t-stat      2 -0.149979   sample_mean_naive  100000  3    uniform   \n",
      "196    t-stat      0 -0.257266  W-decorrelation_15  100000  3    uniform   \n",
      "204    t-stat      1 -0.103867  W-decorrelation_15  100000  3    uniform   \n",
      "212    t-stat      2  1.129231  W-decorrelation_15  100000  3    uniform   \n",
      "268    t-stat  (0,1)  1.017976             uniform  100000  3    uniform   \n",
      "276    t-stat  (0,2)  1.220842             uniform  100000  3    uniform   \n",
      "300    t-stat  (0,1)  0.033558                lvdl  100000  3    uniform   \n",
      "308    t-stat  (0,2)  1.449429                lvdl  100000  3    uniform   \n",
      "316    t-stat  (0,1)  0.436566           two_point  100000  3    uniform   \n",
      "324    t-stat  (0,2)  1.382203           two_point  100000  3    uniform   \n",
      "348    t-stat  (0,1) -0.592489   gamma_exponential  100000  3    uniform   \n",
      "356    t-stat  (0,2)  1.676582   gamma_exponential  100000  3    uniform   \n",
      "364    t-stat  (0,1) -0.592489   sample_mean_naive  100000  3    uniform   \n",
      "372    t-stat  (0,2)  1.676582   sample_mean_naive  100000  3    uniform   \n",
      "\n",
      "     noise_scale  floor_start  floor_decay  initial       dgp  \n",
      "4            1.0     0.333333          0.7        5  nosignal  \n",
      "12           1.0     0.333333          0.7        5  nosignal  \n",
      "20           1.0     0.333333          0.7        5  nosignal  \n",
      "52           1.0     0.333333          0.7        5  nosignal  \n",
      "60           1.0     0.333333          0.7        5  nosignal  \n",
      "68           1.0     0.333333          0.7        5  nosignal  \n",
      "76           1.0     0.333333          0.7        5  nosignal  \n",
      "84           1.0     0.333333          0.7        5  nosignal  \n",
      "92           1.0     0.333333          0.7        5  nosignal  \n",
      "124          1.0     0.333333          0.7        5  nosignal  \n",
      "132          1.0     0.333333          0.7        5  nosignal  \n",
      "140          1.0     0.333333          0.7        5  nosignal  \n",
      "148          1.0     0.333333          0.7        5  nosignal  \n",
      "156          1.0     0.333333          0.7        5  nosignal  \n",
      "164          1.0     0.333333          0.7        5  nosignal  \n",
      "196          1.0     0.333333          0.7        5  nosignal  \n",
      "204          1.0     0.333333          0.7        5  nosignal  \n",
      "212          1.0     0.333333          0.7        5  nosignal  \n",
      "268          1.0     0.333333          0.7        5  nosignal  \n",
      "276          1.0     0.333333          0.7        5  nosignal  \n",
      "300          1.0     0.333333          0.7        5  nosignal  \n",
      "308          1.0     0.333333          0.7        5  nosignal  \n",
      "316          1.0     0.333333          0.7        5  nosignal  \n",
      "324          1.0     0.333333          0.7        5  nosignal  \n",
      "348          1.0     0.333333          0.7        5  nosignal  \n",
      "356          1.0     0.333333          0.7        5  nosignal  \n",
      "364          1.0     0.333333          0.7        5  nosignal  \n",
      "372          1.0     0.333333          0.7        5  nosignal  \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./results/tstat_5c2057b_30148192.pkl', 'rb') as file:\n",
    "    tstat = pickle.load(file)\n",
    "\n",
    "print(tstat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))\n",
    "\n",
    "data.to_csv('./results/data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./results/ys.csv\", ys, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./results/probs.csv\", probs, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./results/rewards.csv\", rewards, delimiter=\",\")\n",
    "np.savetxt(\"./results/arms.csv\", arms, delimiter=\",\")\n",
    "np.savetxt(\"./results/muhat.csv\", muhat, delimiter=\",\")\n",
    "np.savetxt(\"./results/scores.csv\", scores, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
