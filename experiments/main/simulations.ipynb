{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script runs simulations reported in our paper Confidence Intervals for Policy Evaluation in Adaptive Experiments (https://arxiv.org/abs/1911.02768)\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "from sys import argv\n",
    "from random import choice\n",
    "from time import time\n",
    "from os.path import dirname, realpath, join, exists\n",
    "from os import makedirs, chmod\n",
    "from getpass import getuser\n",
    "\n",
    "from adaptive_CI.experiments import run_mab_experiment\n",
    "from adaptive_CI.compute import stick_breaking\n",
    "from adaptive_CI.inference import *\n",
    "from adaptive_CI.weights import *\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Figures\n",
    "\n",
    "This simulation script produces most figures in the paper, with the _exception_ of Figure 1 in the introduction, and its counterpart Figure 13 in Appendix A5.\n",
    "\n",
    "\n",
    "**To non-Stanford members**\n",
    "\n",
    "Each time this script is called, it selects a random configuration (e.g., a signal strength, an experiment horizon, etc) and completes a single simulation using that configuration.\n",
    "\n",
    "In order to produce the figures in our paper, we recommend that this script be run at least $10Ë†5$ times.\n",
    "\n",
    "\n",
    "\n",
    "**To Stanford members with access to the Sherlock cluster**\n",
    "\n",
    "Each time this script is called on sherlock, it selects a random configuration (e.g., a signal strength, an experiment horizon, etc) and completes 200 simulations using that configuration.\n",
    "\n",
    "In order to produce the figures in our paper, we recommend that this script be run a few thousand times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_sherlock():\n",
    "    \"\"\" \n",
    "    Note: This can be ignored by non-Stanford members.\n",
    "\n",
    "    Checks if running on Stanford's Sherlock cluster\n",
    "    \"\"\"\n",
    "    return 'GROUP_SCRATCH' in os.environ\n",
    "\n",
    "\n",
    "def get_sherlock_dir(project, *tail, create=True):\n",
    "    \"\"\"\n",
    "    Note: This can be ignored by non-Stanford members.\n",
    "    \n",
    "    Output consistent folder name in Sherlock.\n",
    "    If create=True and on Sherlock, also makes folder with group permissions.\n",
    "    If create=True and not on Sherlock, does not create anything.\n",
    "\n",
    "    '/scratch/groups/athey/username/project/tail1/tail2/.../tailn'.\n",
    "\n",
    "    >>> get_sherlock_dir('adaptive-inference')\n",
    "    '/scratch/groups/athey/adaptive-inference/vitorh'\n",
    "\n",
    "    >>> get_sherlock_dir('toronto')\n",
    "    '/scratch/groups/athey/toronto/vitorh/'\n",
    "\n",
    "    >>> get_sherlock_dir('adaptive-inference', 'experiments', 'exp_out')\n",
    "    '/scratch/groups/athey/adaptive-inference/vitorh/experiments/exp_out'\n",
    "    \"\"\"\n",
    "    base = join(\"/\", \"scratch\", \"groups\", \"athey\", project, getuser())\n",
    "    path = join(base, *tail)\n",
    "    if not exists(path) and create and on_sherlock():\n",
    "        makedirs(path, exist_ok=True)\n",
    "        # Correct permissions for the whole directory branch\n",
    "        chmod_path = base\n",
    "        chmod(base, 0o775)\n",
    "        for child in tail:\n",
    "            chmod_path = join(chmod_path, child)\n",
    "            chmod(chmod_path, 0o775)\n",
    "    return path\n",
    "\n",
    "\n",
    "def compose_filename(prefix, extension):\n",
    "    \"\"\"\n",
    "    Creates a unique filename based on Github commit id and time.\n",
    "    Useful when running in parallel on server.\n",
    "\n",
    "    INPUT:\n",
    "        - prefix: file name prefix\n",
    "        - extension: file extension\n",
    "\n",
    "    OUTPUT:\n",
    "        - fname: unique filename\n",
    "    \"\"\"\n",
    "    # Tries to find a commit hash\n",
    "    try:\n",
    "        commit = subprocess\\\n",
    "            .check_output(['git', 'rev-parse', '--short', 'HEAD'],\n",
    "                          stderr=subprocess.DEVNULL)\\\n",
    "            .strip()\\\n",
    "            .decode('ascii')\n",
    "    except subprocess.CalledProcessError:\n",
    "        commit = ''\n",
    "\n",
    "    # Other unique identifiers\n",
    "    rnd = str(int(time() * 1e8 % 1e8))\n",
    "    sid = tid = jid = ''\n",
    "    ident = filter(None, [prefix, commit, jid, sid, tid, rnd])\n",
    "    basename = \"_\".join(ident)\n",
    "    fname = f\"{basename}.{extension}\"\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims = 200 if on_sherlock() else 1\n",
    "\n",
    "# DGP specification\n",
    "# ----------------------------------------------------\n",
    "noise_func = 'uniform'\n",
    "truths = {\n",
    "    'nosignal': np.array([1., 1., 1.]),\n",
    "    'lowSNR': np.array([.9, 1., 1.1]),\n",
    "    'highSNR': np.array([.5, 1., 1.5])\n",
    "}\n",
    "if on_sherlock():\n",
    "    Ts = [1_000, 5_000, 10_000, 50_000, 100_000]\n",
    "else:\n",
    "    Ts = [100_000]\n",
    "floor_decays = [.7]\n",
    "initial = 5  # initial number of samples of each arm to do pure exploration\n",
    "exploration = 'TS'\n",
    "noise_scale = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = []\n",
    "df_lambdas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed 20.003150701522827s\n"
     ]
    }
   ],
   "source": [
    "# Run simulations\n",
    "for s in range(num_sims):\n",
    "    if (s+1) % 10 == 0:\n",
    "        print(f'Running simulation {s+1}/{num_sims}')\n",
    "\n",
    "    \"\"\" Experiment configuration \"\"\"\n",
    "    T = choice(Ts)  # number of samples\n",
    "    experiment = choice(list(truths.keys()))\n",
    "    truth = truths[experiment]\n",
    "    K = len(truth)  # number of arms\n",
    "    floor_start = 1/K\n",
    "    floor_decay = choice(floor_decays)\n",
    "\n",
    "    \"\"\" Generate data \"\"\"\n",
    "    noise = np.random.uniform(-noise_scale, noise_scale, size=(T, K))\n",
    "    ys = truth + noise\n",
    "\n",
    "    \"\"\" Run experiment \"\"\"\n",
    "    data = run_mab_experiment(\n",
    "        ys,\n",
    "        initial=initial,\n",
    "        floor_start=floor_start,\n",
    "        floor_decay=floor_decay,\n",
    "        exploration=exploration)\n",
    "\n",
    "    probs = data['probs']\n",
    "    rewards = data['rewards']\n",
    "    arms = data['arms']\n",
    "\n",
    "    \"\"\" Compute AIPW scores \"\"\"\n",
    "    muhat = np.row_stack([np.zeros(K), sample_mean(rewards, arms, K)[:-1]])\n",
    "    scores = aw_scores(rewards, arms, probs, muhat)\n",
    "\n",
    "    \"\"\" Compute weights \"\"\"\n",
    "    # Two-point allocation rate\n",
    "    twopoint_ratio = twopoint_stable_var_ratio(e=probs, alpha=floor_decay)\n",
    "    twopoint_ratio_old = twopoint_stable_var_ratio_old(probs, floor_start, floor_decay)\n",
    "    twopoint_h2es = stick_breaking(twopoint_ratio)\n",
    "    twopoint_h2es_old = stick_breaking(twopoint_ratio_old)\n",
    "    wts_twopoint = np.sqrt(np.maximum(0., twopoint_h2es * probs))\n",
    "\n",
    "    # Other weights: lvdl(constant allocation rate), propscore and uniform\n",
    "    wts_lvdl = np.sqrt(probs)\n",
    "    wts_propscore = probs\n",
    "    wts_uniform = np.ones_like(probs)\n",
    "\n",
    "    \"\"\" Estimate arm values \"\"\"\n",
    "    # for each weighting scheme, return [estimate, S.E, bias, 90%-coverage, t-stat, mse, truth]\n",
    "    stats = dict(\n",
    "        uniform=evaluate_aipw_stats(scores, wts_uniform, truth),\n",
    "        propscore=evaluate_aipw_stats(scores, wts_propscore, truth),\n",
    "        lvdl=evaluate_aipw_stats(scores, wts_lvdl, truth),\n",
    "        two_point=evaluate_aipw_stats(scores, wts_twopoint, truth),\n",
    "        beta_bernoulli=evaluate_beta_bernoulli_stats(rewards, arms, truth, K, floor_decay, alpha=.1),\n",
    "        gamma_exponential=evaluate_gamma_exponential_stats(rewards, arms, truth, K, floor_decay, c=2, expected_noise_variance=1/3, alpha=.1),\n",
    "        sample_mean_naive=evaluate_sample_mean_naive_stats(rewards, arms, truth, K, alpha=.1)\n",
    "    )\n",
    "    \n",
    "    # # add estimates of W_decorrelation\n",
    "    W_name = f'wdecorr_results/W_lambdas_{experiment}-{noise_func}-{T}-{floor_decay}.npz'\n",
    "    try:\n",
    "        W_save = np.load(W_name)  # load presaved W-lambdas\n",
    "        for percentile, W_lambda in zip(W_save['percentiles'], W_save['W_lambdas']):\n",
    "            stats[f'W-decorrelation_{percentile}'] = wdecorr_stats(arms, rewards, K, W_lambda, truth)\n",
    "    except FileNotFoundError:\n",
    "        print(f'Could not find relevant w-decorrelation file {W_name}. Ignoring.')\n",
    "        \n",
    "    \n",
    "    \"\"\" Estimate contrasts \"\"\"\n",
    "    contrasts = dict(\n",
    "        uniform=evaluate_aipw_contrasts(scores, wts_uniform, truth),\n",
    "        propscore=evaluate_aipw_contrasts(scores, wts_propscore, truth),\n",
    "        lvdl=evaluate_aipw_contrasts(scores, wts_lvdl, truth),\n",
    "        two_point=evaluate_aipw_contrasts(scores, wts_twopoint, truth),\n",
    "        beta_bernoulli=evaluate_beta_bernoulli_contrasts(rewards, arms, truth, K, floor_decay, alpha=.1),\n",
    "        gamma_exponential=evaluate_gamma_exponential_contrasts(rewards, arms, truth, K, floor_decay, c=2, expected_noise_variance=1/3, alpha=.1),\n",
    "        sample_mean_naive=evaluate_sample_mean_naive_contrasts(rewards, arms, truth, K, alpha=.1)\n",
    "    )\n",
    "\n",
    "    \n",
    "    \"\"\" Save results \"\"\"\n",
    "    config = dict(\n",
    "        T=T,\n",
    "        K=K,\n",
    "        noise_func=noise_func,\n",
    "        noise_scale=noise_scale,\n",
    "        floor_start=floor_start,\n",
    "        floor_decay=floor_decay,\n",
    "        initial=initial,\n",
    "        dgp=experiment,\n",
    "    )\n",
    "\n",
    "    ratios = dict(\n",
    "        lvdl=np.ones((T, K)) / np.arange(T, 0, -1)[:, np.newaxis],\n",
    "        two_point=twopoint_ratio,\n",
    "    )\n",
    "    \n",
    "    # save lambda values at selected timepoints\n",
    "    saved_timepoints = list(range(0, T, T // 250)) + [T-1]\n",
    "    for ratio in ratios:\n",
    "        ratios[ratio] = ratios[ratio][saved_timepoints, :]\n",
    "    \n",
    "    # tabulate arm values\n",
    "    tabs_stats = []\n",
    "    for method, stat in stats.items():\n",
    "        tab_stats = pd.DataFrame({\"statistic\": [\"estimate\", \"stderr\", \"bias\", \"90% coverage of t-stat\", \"t-stat\", \"mse\", \"CI_width\", \"truth\"] * stat.shape[1],\n",
    "                                  \"policy\": np.repeat(np.arange(K), stat.shape[0]),\n",
    "                                  \"value\":  stat.flatten(order='F'),\n",
    "                                  \"method\": method,\n",
    "                                 **config})\n",
    "        tabs_stats.append(tab_stats)\n",
    "\n",
    "\n",
    "    # tabulate arm contrasts\n",
    "    tabs_contrasts = []\n",
    "    for method, contrast in contrasts.items():\n",
    "        tabs_contrast = pd.DataFrame({\"statistic\": [\"estimate\", \"stderr\", \"bias\", \"90% coverage of t-stat\", \"t-stat\", \"mse\", \"CI_width\", \"truth\"] * contrast.shape[1],\n",
    "                                      \"policy\": np.repeat([f\"(0,{k})\" for k in np.arange(1, K)], contrast.shape[0]),\n",
    "                                      \"value\": contrast.flatten(order='F'),\n",
    "                                      \"method\": method,\n",
    "                                     **config})\n",
    "        tabs_contrasts.append(tabs_contrast)\n",
    "\n",
    "    \n",
    "    df_stats.extend(tabs_stats)\n",
    "    df_stats.extend(tabs_contrasts)\n",
    "    \n",
    "    \n",
    "    \"\"\" Save relevant lambda weights, if applicable \"\"\"\n",
    "    if T == max(Ts):\n",
    "        saved_timepoints = list(range(0, T, T // 500))\n",
    "        lambdas = twopoint_ratio[saved_timepoints] * (T - np.array(saved_timepoints)[:,np.newaxis])\n",
    "        lambdas = {key: value for key, value in enumerate(lambdas.T)}\n",
    "        dfl = pd.DataFrame({**lambdas, **config, 'time': saved_timepoints})\n",
    "        dfl = pd.melt(dfl, id_vars=list(config.keys()) + ['time'], var_name='policy', value_vars=list(range(K)))\n",
    "        df_lambdas.append(dfl)\n",
    "        \n",
    "    print(f\"Time passed {time()-start_time}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        sample_mean_naive=evaluate_sample_mean_naive_contrasts(rewards, arms, truth, K, alpha=.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f9ba952eb10a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_lambdas\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf_lambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_lambdas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "df_stats = pd.concat(df_stats)\n",
    "if len(df_lambdas) > 0:\n",
    "    df_lambdas = pd.concat(df_lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if on_sherlock():\n",
    "    write_dir = get_sherlock_dir('adaptive-confidence-intervals', 'simulations', create=True)\n",
    "    print(f\"saving at {write_dir}\")\n",
    "else:\n",
    "     write_dir = join(os.getcwd(), 'results')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.concat(df_stats, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving information about contrasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_contrast = compose_filename(f'contrast', 'pkl')\n",
    "write_path_contrast = os.path.join(write_dir, filename_contrast)\n",
    "\n",
    "df_contrast = df_stats.query(\n",
    "            'policy == \"(0,2)\" and '\n",
    "            'statistic == [\"mse\", \"bias\", \"90% coverage of t-stat\", \"CI_width\"] and '\n",
    "            \"method == ['uniform', 'lvdl', 'two_point',  'sample_mean_naive', 'gamma_exponential', 'W-decorrelation_15']\")\n",
    "df_contrast.to_pickle(write_path_contrast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save information about arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_arms = compose_filename(f'arm', 'pkl')\n",
    "write_path_arms = os.path.join(write_dir, filename_arms)\n",
    "\n",
    "df_arms = df_stats.query(\n",
    "            'policy == [0, 1, 2] and '\n",
    "            'statistic == [\"mse\", \"bias\", \"90% coverage of t-stat\", \"CI_width\"] and '\n",
    "            \"method == ['uniform', 'lvdl', 'two_point',  'sample_mean_naive', 'gamma_exponential', 'W-decorrelation_15']\")\n",
    "df_arms.to_pickle(write_path_arms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save information about \"t-stats\" (i.e., our studentized 'statistics')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_tstats = compose_filename(f'tstat', 'pkl')\n",
    "write_path_tstats = os.path.join(write_dir, filename_tstats)\n",
    "\n",
    "T_max = max(Ts)\n",
    "df_tstats = df_stats.query(\n",
    "    \"method == ['uniform', 'lvdl', 'two_point',  'sample_mean_naive', 'gamma_exponential', 'W-decorrelation_15'] and \"\n",
    "    \"T == @T_max and \"\n",
    "    \"statistic == 't-stat'\"\n",
    ")\n",
    "df_tstats.to_pickle(write_path_tstats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save information about $\\lambda$ behavior, if appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_lambdas = compose_filename(f'lambdas', 'pkl')\n",
    "write_path_lambdas = os.path.join(write_dir, filename_lambdas)\n",
    "if len(df_lambdas) > 0:\n",
    "    df_lambdas = pd.concat(df_lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done.\n"
     ]
    }
   ],
   "source": [
    "print(\"All done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
