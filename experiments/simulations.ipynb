{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script runs simulations reported in our paper Confidence Intervals for Policy Evaluation in Adaptive Experiments (https://arxiv.org/abs/1911.02768)\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "# sys.path.insert(0, \"/home/rhzhan/adaptive-confidence-intervals/\")\n",
    "from time import time\n",
    "from sys import argv\n",
    "from random import choice\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from adaptive_CI.experiments import run_mab_experiment\n",
    "from adaptive_CI.compute import stick_breaking\n",
    "from adaptive_CI.saving import *\n",
    "from adaptive_CI.inference import *\n",
    "from adaptive_CI.weights import *\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from time import time\n",
    "from os.path import dirname, realpath, join, exists\n",
    "from os import makedirs, chmod\n",
    "from getpass import getuser\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_sherlock():\n",
    "    \"\"\" Checks if running locally or on sherlock \"\"\"\n",
    "    return 'GROUP_SCRATCH' in os.environ\n",
    "\n",
    "\n",
    "def get_sherlock_dir(project, *tail, create=True):\n",
    "    \"\"\"\n",
    "    Output consistent folder name in Sherlock.\n",
    "    If create=True and on Sherlock, also makes folder with group permissions.\n",
    "    If create=True and not on Sherlock, does not create anything.\n",
    "\n",
    "    '/scratch/groups/athey/username/project/tail1/tail2/.../tailn'.\n",
    "\n",
    "    >>> get_sherlock_dir('adaptive-inference')\n",
    "    '/scratch/groups/athey/adaptive-inference/vitorh'\n",
    "\n",
    "    >>> get_sherlock_dir('toronto')\n",
    "    '/scratch/groups/athey/toronto/vitorh/'\n",
    "\n",
    "    >>> get_sherlock_dir('adaptive-inference', 'experiments', 'exp_out')\n",
    "    '/scratch/groups/athey/adaptive-inference/vitorh/experiments/exp_out'\n",
    "    \"\"\"\n",
    "    base = join(\"/\", \"scratch\", \"groups\", \"athey\", project, getuser())\n",
    "    path = join(base, *tail)\n",
    "    if not exists(path) and create and on_sherlock():\n",
    "        makedirs(path, exist_ok=True)\n",
    "        # Correct permissions for the whole directory branch\n",
    "        chmod_path = base\n",
    "        chmod(base, 0o775)\n",
    "        for child in tail:\n",
    "            chmod_path = join(chmod_path, child)\n",
    "            chmod(chmod_path, 0o775)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "num_sims = 50 if on_sherlock() else 1\n",
    "\n",
    "# Read DGP specification\n",
    "noise_func = 'uniform'\n",
    "truths = {\n",
    "    'nosignal': np.array([1., 1., 1.]),\n",
    "    'lowSNR': np.array([.9, 1., 1.1]),\n",
    "    'highSNR': np.array([.5, 1., 1.5])\n",
    "}\n",
    "\n",
    "results_list = []\n",
    "start_time = time()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Run simulations\n",
    "for s in range(num_sims):\n",
    "    if (s+1) % 10 == 0:\n",
    "        print(f'Running simulation {s+1}/{num_sims}')\n",
    "\n",
    "    \"\"\" Experiment configuration \"\"\"\n",
    "    T = choice([1000, 5000, 10000, 50000, 100_000])  # number of samples\n",
    "    experiment = choice(list(truths.keys()))\n",
    "    truth = truths[experiment]\n",
    "    K = len(truth)  # number of arms\n",
    "    initial = 5  # initial number of samples of each arm to do pure exploration\n",
    "    floor_start = 1/K\n",
    "    floor_decay = choice([.25, .5, .6, .7, .8, .9, .99])\n",
    "    exploration = 'TS'\n",
    "    noise_scale = 1.0\n",
    "\n",
    "    \"\"\" Generate data \"\"\"\n",
    "    if noise_func == 'uniform':\n",
    "        noise = np.random.uniform(-noise_scale, noise_scale, size=(T, K))\n",
    "        R = noise_scale * 2\n",
    "    else:\n",
    "        noise = np.random.exponential(noise_scale, size=(T, K)) - noise_scale\n",
    "        R = -np.log(0.001) * noise_scale\n",
    "    ys = truth + noise\n",
    "\n",
    "    \"\"\" Run experiment \"\"\"\n",
    "    data = run_mab_experiment(\n",
    "        ys,\n",
    "        initial=initial,\n",
    "        floor_start=floor_start,\n",
    "        floor_decay=floor_decay,\n",
    "        exploration=exploration)\n",
    "\n",
    "    probs = data['probs']\n",
    "    rewards = data['rewards']\n",
    "    arms = data['arms']\n",
    "\n",
    "    \"\"\" Compute AIPW scores \"\"\"\n",
    "    muhat = np.row_stack([np.zeros(K), sample_mean(rewards, arms, K)[:-1]])\n",
    "    scores = aw_scores(rewards, arms, probs, muhat)\n",
    "\n",
    "    \"\"\" Compute weights \"\"\"\n",
    "    # Two-point allocation rate\n",
    "    twopoint_ratio = twopoint_stable_var_ratio(e=probs, alpha=floor_decay)\n",
    "    twopoint_ratio_old = twopoint_stable_var_ratio_old(probs, floor_start, floor_decay)\n",
    "    twopoint_h2es = stick_breaking(twopoint_ratio)\n",
    "    twopoint_h2es_old = stick_breaking(twopoint_ratio_old)\n",
    "    wts_twopoint = np.sqrt(np.maximum(0., twopoint_h2es * probs))\n",
    "    wts_twopoint_old = np.sqrt(np.maximum(0., twopoint_h2es_old * probs))\n",
    "\n",
    "    # Other weights: lvdl(constant allocation rate), propscore and uniform\n",
    "    wts_lvdl = np.sqrt(probs)\n",
    "    wts_propscore = probs\n",
    "    wts_uniform = np.ones_like(probs)\n",
    "\n",
    "    \"\"\" Estimate arm values \"\"\"\n",
    "    # for each weighting scheme, return [estimate, S.E, bias, 95%-coverage, t-stat, mse, truth]\n",
    "    stats = dict(\n",
    "        uniform=aw_stats(scores, wts_uniform, truth),\n",
    "        propscore=aw_stats(scores, wts_propscore, truth),\n",
    "        lvdl=aw_stats(scores, wts_lvdl, truth),\n",
    "        two_point=aw_stats(scores, wts_twopoint, truth),\n",
    "        two_point_old=aw_stats(scores, wts_twopoint_old, truth),\n",
    "        bernstein=population_bernstein_stats(rewards, arms, truth, K),\n",
    "        #empirical_bernstein=empirical_bernstein_stats(rewards, arms, truth, K, R),\n",
    "    )\n",
    "\n",
    "    # # add estimates of W_decorrelation\n",
    "    # W_names = f'W_lambdas_{experiment}-{noise_func}-{T}.npz'\n",
    "    # W_save = np.load(W_names)  # load presaved W-lambdas\n",
    "    # for percentile, W_lambda in zip(W_save['percentiles'], W_save['W_lambdas']):\n",
    "    #     stats[f'W-decorrelation_{percentile}'] = wdecorr_stats(\n",
    "    #         arms, rewards, K, W_lambda, truth)\n",
    "\n",
    "    \"\"\" Estimate contrasts \"\"\"\n",
    "    contrasts = dict(\n",
    "        uniform=aw_contrasts(scores, wts_uniform, truth),\n",
    "        propscore=aw_contrasts(scores, wts_propscore, truth),\n",
    "        lvdl=aw_contrasts(scores, wts_lvdl, truth),\n",
    "        two_point=aw_contrasts(scores, wts_twopoint, truth),\n",
    "        two_point_old=aw_contrasts(scores, wts_twopoint_old, truth),\n",
    "        bernstein=population_bernstein_contrast(rewards, arms, truth, K)\n",
    "    )\n",
    "\n",
    "    \"\"\" Save results \"\"\"\n",
    "    weights = dict(\n",
    "        uniform=wts_uniform,\n",
    "        propscore=wts_propscore,\n",
    "        lvdl=wts_lvdl,\n",
    "        two_point=wts_twopoint,\n",
    "    )\n",
    "\n",
    "    ratios = dict(\n",
    "        lvdl=np.ones((T, K)) / np.arange(T, 0, -1)[:, np.newaxis],\n",
    "        two_point=twopoint_ratio,\n",
    "    )\n",
    "\n",
    "    config = dict(\n",
    "        T=T,\n",
    "        K=K,\n",
    "        noise_func=noise_func,\n",
    "        noise_scale=noise_scale,\n",
    "        floor_start=floor_start,\n",
    "        floor_decay=floor_decay,\n",
    "        initial=initial,\n",
    "        dgp=experiment,\n",
    "    )\n",
    "\n",
    "#     # only save at saved_timepoints for assigmnment probabilities, conditional variance, weights, ratios(lambdas)\n",
    "#     saved_timepoints = list(range(0, T, T//100))\n",
    "#     condVars = dict()\n",
    "#     for method, weight in weights.items():\n",
    "#         condVar = weight ** 2 / probs / np.sum(weight, 0) ** 2 * T\n",
    "#         weight = weight / np.sum(weight, 0) * T\n",
    "#         condVars[method] = condVar[saved_timepoints, :]\n",
    "#         weights[method] = weight[saved_timepoints, :]\n",
    "#     for ratio in ratios:\n",
    "#         ratios[ratio] = ratios[ratio][saved_timepoints, :]\n",
    "#     probs = probs[saved_timepoints, :]\n",
    "\n",
    "    results = dict(\n",
    "        config=config,\n",
    "        probs=probs,\n",
    "        stats=stats,\n",
    "        contrasts=contrasts,\n",
    "        weights=weights,\n",
    "#         ratios=ratios,\n",
    "#         condVars=condVars\n",
    "    )\n",
    "    \n",
    "    r = results\n",
    "    timepoints = np.arange(0, T, T//100)\n",
    "\n",
    "    # get statistics table\n",
    "    tabs_stats = []\n",
    "    for method, stat in r['stats'].items():\n",
    "        stat = np.row_stack([stat, np.abs(stat[2])])\n",
    "        tab_stats = pd.DataFrame({\"statistic\": [\"estimate\", \"stderr\", \"bias\", \"90% coverage of t-stat\", \"t-stat\", \"mse\", \"CI_width\", \"truth\", 'abserr'] * stat.shape[1],\n",
    "                                  \"policy\": np.repeat(np.arange(K), stat.shape[0]),\n",
    "                                  \"value\":  stat.flatten(order='F'),\n",
    "                                  \"method\": method,\n",
    "                                 **r['config']})\n",
    "        tabs_stats.append(tab_stats)\n",
    "\n",
    "\n",
    "    # get contrast table\n",
    "    tabs_contrasts = []\n",
    "    for method, contrast in r['contrasts'].items():\n",
    "        tabs_contrast = pd.DataFrame({\"statistic\": [\"truth\",\n",
    "                                                    \"estimate\", \"bias\", \"mse\",\n",
    "                                                    \"stderr\", \"t-stat\", \"90% coverage of t-stat\", \"CI_width\"] * contrast.shape[1],\n",
    "                                      \"policy\": np.repeat([f\"(0,{k})\" for k in np.arange(1, K)], contrast.shape[0]),\n",
    "                                      \"value\": contrast.flatten(order='F'),\n",
    "                                      \"method\": method,\n",
    "                                     **r['config']})\n",
    "        tabs_contrasts.append(tabs_contrast)\n",
    "\n",
    "    df = pd.concat(tabs_stats + tabs_contrasts)\n",
    "    \n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed 8.122915983200073s\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(dfs)\n",
    "\n",
    "filename = compose_filename(f'weight_experiment_{experiment}_{noise_func}', 'pkl')\n",
    "\n",
    "if on_sherlock():\n",
    "    write_dir = get_sherlock_dir('adaptive-confidence-intervals', 'simulations', create=True)\n",
    "    print(f\"saving at {write_dir}\")\n",
    "else:\n",
    "     write_dir = join(os.getcwd(), 'results')\n",
    "write_path = os.path.join(write_dir, filename)\n",
    "df.to_pickle(write_path)\n",
    "    \n",
    "print(f\"Time passed {time()-start_time}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
